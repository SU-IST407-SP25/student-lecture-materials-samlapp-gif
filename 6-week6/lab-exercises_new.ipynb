{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will extend a base `Animal` class to create a more specific `Dog` class. This will help you practice inheritance and method overriding in Python.\n",
    "\n",
    "#### Task:\n",
    "\n",
    "1. Review the provided `Animal` class.\n",
    "2. Create a `Dog` class that inherits from `Animal`.\n",
    "3. Override the `make_sound` method to return \"Woof!\" instead of the generic animal sound.\n",
    "4. Add a new method called `fetch` that returns a string saying the dog's name and \"fetched the ball!\".\n",
    "\n",
    "Here's the base `Animal` class to start with:\n",
    "\n",
    "```python\n",
    "class Animal:\n",
    "    def __init__(self, name, species):\n",
    "        self.name = name\n",
    "        self.species = species\n",
    "    \n",
    "    def make_sound(self):\n",
    "        return \"Some generic animal sound\"\n",
    "\n",
    "    def describe(self):\n",
    "        return f\"{self.name} is a {self.species}\"\n",
    "```\n",
    "\n",
    "Your task is to create the `Dog` class below this `Animal` class definition.\n",
    "\n",
    "#### Requirements:\n",
    "\n",
    "- The `Dog` class should inherit from `Animal`.\n",
    "- The `Dog` class should have its own `__init__` method that calls the parent class's `__init__` method.\n",
    "- Override the `make_sound` method to return \"Woof!\".\n",
    "- Implement a new `fetch` method as described above.\n",
    "\n",
    "#### Hint:\n",
    "\n",
    "Remember to use the `super()` function to call methods from the parent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will create a `LogTransform` class that applies a logarithmic transformation to input data. This class should be compatible with scikit-learn's transformer interface and include an inverse transform method.\n",
    "\n",
    "#### Task:\n",
    "\n",
    "Create a `LogTransform` class with the following methods:\n",
    "1. `__init__(self, base=np.e)`: Initialize the transformer with a logarithm base (default to natural log).\n",
    "2. `fit(self, X, y=None)`: This method should just return self (as log transform doesn't need fitting).\n",
    "3. `transform(self, X)`: Apply log transformation to the input data.\n",
    "4. `inverse_transform(self, X)`: Reverse the log transformation.\n",
    "\n",
    "#### Requirements:\n",
    "\n",
    "- The class should inherit from `BaseEstimator` and `TransformerMixin` from scikit-learn.\n",
    "- Handle potential errors, such as non-positive values in the input data.\n",
    "- Ensure that the `transform` and `inverse_transform` methods work correctly with both 1D and 2D numpy arrays.\n",
    "\n",
    "#### Hint:\n",
    "\n",
    "Remember to import necessary modules (numpy, and classes from scikit-learn). Use `np.log` and `np.exp` for the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Implement this class\n",
    "class LogTransform:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create some sample data\n",
    "X = np.linspace(0.1, 10, 100).reshape(-1, 1)\n",
    "\n",
    "# Initialize and use the LogTransform\n",
    "log_transformer = LogTransform(base=np.e)\n",
    "X_transformed = log_transformer.fit_transform(X)\n",
    "X_inverse = log_transformer.inverse_transform(X_transformed)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.plot(X, X)\n",
    "plt.title(\"Original Data\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.plot(X, X_transformed)\n",
    "plt.title(\"Log Transformed Data\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Log(Y)\")\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(X, X_inverse)\n",
    "plt.title(\"Inverse Transformed Data\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verify that inverse_transform reverses transform\n",
    "np.testing.assert_allclose(X, X_inverse)\n",
    "print(\"Transformation and inverse transformation verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement a simple gradient descent algorithm to find the minimum of a quadratic function. This will help you understand the core concepts of gradient descent before applying it to linear regression.\n",
    "\n",
    "#### Task:\n",
    "\n",
    "1. Implement a function `quadratic(x)` that returns the value of the quadratic function f(x) = x^2 + 2x + 1.\n",
    "2. Implement a function `quadratic_derivative(x)` that returns the derivative of the quadratic function.\n",
    "3. Implement a `gradient_descent` function that uses these functions to find the minimum of the quadratic function.\n",
    "4. Visualize the progress of the gradient descent algorithm.\n",
    "\n",
    "#### Requirements:\n",
    "\n",
    "- The `gradient_descent` function should take the following parameters:\n",
    "  - `start`: The starting point for x\n",
    "  - `learning_rate`: The step size for each iteration\n",
    "  - `num_iterations`: The number of iterations to run\n",
    "  - `tolerance`: Stop if the change in x is smaller than this value\n",
    "- Plot the quadratic function and show the path taken by gradient descent.\n",
    "- Print the final minimum point found by the algorithm.\n",
    "\n",
    "#### Hint:\n",
    "\n",
    "The derivative of x^2 + 2x + 1 is 2x + 2. The update rule for gradient descent is:\n",
    "x_new = x_old - learning_rate * derivative(x_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def quadratic(x):\n",
    "    pass\n",
    "    # implement me\n",
    "\n",
    "def quadratic_derivative(x):\n",
    "    pass\n",
    "    # Implement me\n",
    "\n",
    "def gradient_descent(start, learning_rate, num_iterations, tolerance):\n",
    "    pass\n",
    "    # Should return x, x_history\n",
    "\n",
    "# Set parameters\n",
    "start = 2.0\n",
    "learning_rate = 0.1\n",
    "num_iterations = 100\n",
    "tolerance = 1e-6\n",
    "\n",
    "# Run gradient descent\n",
    "minimum, x_history = gradient_descent(start, learning_rate, num_iterations, tolerance)\n",
    "\n",
    "# Prepare data for plotting\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = quadratic(x)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, 'b-', label='f(x) = x^2 + 2x + 1')\n",
    "plt.plot(x_history, [quadratic(x) for x in x_history], 'ro-', label='Gradient Descent Path')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Gradient Descent on a Quadratic Function')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Add annotations\n",
    "for i, (x, y) in enumerate(zip(x_history, [quadratic(x) for x in x_history])):\n",
    "    if i % 5 == 0:  # Annotate every 5th point to avoid clutter\n",
    "        plt.annotate(f'Step {i}', (x, y), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"The minimum point found is x = {minimum:.6f}\")\n",
    "print(f\"The value of f(x) at this point is {quadratic(minimum):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement a Simple Linear Regression class that is compatible with scikit-learn's estimator interface. You'll use gradient descent for optimization, as discussed in the lecture notes.\n",
    "\n",
    "#### Task:\n",
    "\n",
    "Create a `SimpleLinearRegression` class with the following methods:\n",
    "1. `__init__(self, learning_rate=0.01, n_iterations=1000, tolerance=1e-6)`: Initialize the model parameters.\n",
    "2. `fit(self, X, y)`: Fit the model to the training data using gradient descent.\n",
    "3. `predict(self, X)`: Make predictions using the trained model.\n",
    "4. `score(self, X, y)`: Calculate the coefficient of determination R^2 of the prediction.\n",
    "\n",
    "#### Requirements:\n",
    "\n",
    "- The class should inherit from `BaseEstimator` and `RegressorMixin` from scikit-learn.\n",
    "- Use gradient descent to optimize the parameters (weight and bias).\n",
    "- Store the weight as `self.coef_` and the bias as `self.intercept_` (note the trailing underscores).\n",
    "- Implement early stopping in the `fit` method using the `tolerance` parameter.\n",
    "- Ensure that the `fit`, `predict`, and `score` methods work with both 1D and 2D numpy arrays for X.\n",
    "\n",
    "#### Hints:\n",
    "\n",
    "- Remember to reshape input arrays if necessary to ensure consistent dimensions.\n",
    "- The gradient descent update rules for simple linear regression are:\n",
    "  - w = w - learning_rate * (1/n) * sum((y_pred - y) * x)\n",
    "  - b = b - learning_rate * (1/n) * sum(y_pred - y)\n",
    "- You can use `np.mean((y_true - y_pred) ** 2)` to calculate MSE for the stopping criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array\n",
    "\n",
    "class SimpleLinearRegression:\n",
    "    #Implement me\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = SimpleLinearRegression(learning_rate=0.01, n_iterations=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print R-squared score\n",
    "print(f\"R-squared score: {model.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual')\n",
    "plt.plot(X_test, y_pred, color='red', label='Predicted')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Simple Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Coefficient: {model.coef_[0]:.4f}\")\n",
    "print(f\"Intercept: {model.intercept_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will extend the Simple Linear Regression implementation from Exercise 4 to create a Multiple Linear Regression class. This class will also be compatible with scikit-learn's estimator interface and use gradient descent for optimization.\n",
    "\n",
    "#### Task:\n",
    "\n",
    "Create a `MultipleLinearRegression` class with the following methods:\n",
    "1. `__init__(self, learning_rate=0.01, n_iterations=1000, tolerance=1e-6)`: Initialize the model parameters.\n",
    "  - the \"tolerance\" parameter is used as a stopping criterion for the gradient descent algorithm. It helps determine when the algorithm should stop iterating, based on how much the model parameters are changing between iterations.\n",
    "  - after each iteration in gradient descent, if the absolute value of the change in coefficients is less than tolerance, we can halt the gradient descent process\n",
    "2. `fit(self, X, y)`: Fit the model to the training data using gradient descent.\n",
    "3. `predict(self, X)`: Make predictions using the trained model.\n",
    "4. `score(self, X, y)`: Calculate the coefficient of determination R^2 of the prediction.\n",
    "\n",
    "#### Requirements:\n",
    "\n",
    "- The class should inherit from `BaseEstimator` and `RegressorMixin` from scikit-learn.\n",
    "- Use gradient descent to optimize the parameters (weights and bias).\n",
    "- Store the weights as `self.coef_` and the bias as `self.intercept_` (note the trailing underscores).\n",
    "- Implement early stopping in the `fit` method using the `tolerance` parameter.\n",
    "- Ensure that the `fit`, `predict`, and `score` methods work with 2D numpy arrays for X.\n",
    "- Handle multiple features in the input data.\n",
    "\n",
    "#### Hints:\n",
    "\n",
    "- The implementation will be very similar to the Simple Linear Regression class, but you'll need to handle multiple features.\n",
    "- The gradient descent update rules for multiple linear regression are:\n",
    "  - w = w - learning_rate * (1/n) * X.T.dot(y_pred - y)\n",
    "  - b = b - learning_rate * (1/n) * sum(y_pred - y)\n",
    "- You can use `np.mean((y_true - y_pred) ** 2)` to calculate MSE for the stopping criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array\n",
    "\n",
    "class MultipleLinearRegression:\n",
    "    # implement me\n",
    "    pass\n",
    "\n",
    "# Test the implementation\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=3, noise=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = MultipleLinearRegression(learning_rate=0.01, n_iterations=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print R-squared score\n",
    "print(f\"R-squared score: {model.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Plot results (for the first feature)\n",
    "plt.scatter(X_test[:, 0], y_test, color='blue', label='Actual')\n",
    "plt.scatter(X_test[:, 0], y_pred, color='red', label='Predicted')\n",
    "plt.xlabel('X (first feature)')\n",
    "plt.ylabel('y')\n",
    "plt.title('Multiple Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(f\"Intercept: {model.intercept_:.4f}\")\n",
    "\n",
    "# Compare with sklearn's LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "sk_model = LinearRegression()\n",
    "sk_model.fit(X_train, y_train)\n",
    "sk_score = sk_model.score(X_test, y_test)\n",
    "print(f\"sklearn LinearRegression R-squared score: {sk_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
